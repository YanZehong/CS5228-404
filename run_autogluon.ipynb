{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0512bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df9963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "def check_nan(df, k):\n",
    "    total_na = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = ((df.isnull().sum() / df.isnull().count()) * 100).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total_na, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data = missing_data.reset_index()\n",
    "    missing_data.columns = ['Name', 'Total', 'Percent']\n",
    "    print(missing_data[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c86296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat different aspects into DataFrame\n",
    "def concat_aspect(df_train, df_test, aspect):\n",
    "    if aspect == 'schools':\n",
    "        schools01 = pd.read_csv('data/primary_school_01.csv', sep=',')\n",
    "        schools01_test = pd.read_csv('data/primary_school_01_test.csv', sep=',')\n",
    "        schools02 = pd.read_csv('data/primary_school_02.csv', sep=',')\n",
    "        schools02_test = pd.read_csv('data/primary_school_02_test.csv', sep=',')\n",
    "        schools03 = pd.read_csv('data/primary_school_03.csv', sep=',')\n",
    "        schools03_test = pd.read_csv('data/primary_school_03_test.csv', sep=',')\n",
    "\n",
    "        secschool01 = pd.read_csv('data/sec_school_01.csv', sep=',')\n",
    "        secschool01_test = pd.read_csv('data/sec_school_01_test.csv', sep=',')\n",
    "        secschool02 = pd.read_csv('data/sec_school_02.csv', sep=',')\n",
    "        secschool02_test = pd.read_csv('data/sec_school_02_test.csv', sep=',')\n",
    "        secschool03 = pd.read_csv('data/sec_school_03.csv', sep=',')\n",
    "        secschool03_test = pd.read_csv('data/sec_school_03_test.csv', sep=',')\n",
    "        \n",
    "        train_data = pd.concat([df_train[df_train.columns.values[:-1]], \n",
    "                                schools01, schools02, schools03, \n",
    "                                secschool01, secschool02, secschool03,\n",
    "                                df_train['price']], axis=1)\n",
    "        test_data = pd.concat([df_test, schools01_test, schools02_test, schools03_test, \n",
    "                               secschool01_test, secschool02_test, secschool03_test], axis=1)\n",
    "        # check_nan(train_data, 5)\n",
    "        # check_nan(test_data, 5)\n",
    "        train_data.to_csv('data/train_concat_schools.csv', index = False)\n",
    "        test_data.to_csv('data/test_concat_schools.csv', index = False)\n",
    "        \n",
    "    elif aspect == 'amenities':\n",
    "        shopping01 = pd.read_csv('data/shopping_01.csv', sep=',')\n",
    "        shopping01_test = pd.read_csv('data/shopping_01_test.csv', sep=',')\n",
    "        shopping02 = pd.read_csv('data/shopping_02.csv', sep=',')\n",
    "        shopping02_test = pd.read_csv('data/shopping_02_test.csv', sep=',')\n",
    "        shopping03 = pd.read_csv('data/shopping_03.csv', sep=',')\n",
    "        shopping03_test = pd.read_csv('data/shopping_03_test.csv', sep=',')\n",
    "        \n",
    "        hawker_01 = pd.read_csv('data/hawker_01.csv', sep=',')\n",
    "        hawker_01_test = pd.read_csv('data/hawker_01_test.csv', sep=',')\n",
    "        hawker_02 = pd.read_csv('data/hawker_02.csv', sep=',')\n",
    "        hawker_02_test = pd.read_csv('data/hawker_02_test.csv', sep=',')\n",
    "        hawker_03 = pd.read_csv('data/hawker_03.csv', sep=',')\n",
    "        hawker_03_test = pd.read_csv('data/hawker_03_test.csv', sep=',')\n",
    "        \n",
    "        train_data = pd.concat([df_train[df_train.columns.values[:-1]], \n",
    "                                shopping01, shopping02, shopping03, \n",
    "                                hawker_01, hawker_02, hawker_03,\n",
    "                                df_train['price']], axis=1)\n",
    "        test_data = pd.concat([df_test, shopping01_test, shopping02_test, shopping03_test, \n",
    "                               hawker_01_test, hawker_02_test, hawker_03_test], axis=1)\n",
    "        # check_nan(train_data, 5)\n",
    "        # check_nan(test_data, 5)\n",
    "        train_data.to_csv('data/train_concat_amenities.csv', index = False)\n",
    "        test_data.to_csv('data/test_concat_amenities.csv', index = False)\n",
    "        \n",
    "    elif aspect == 'location':\n",
    "        cc01 = pd.read_csv('data/cc_01.csv', sep=',')\n",
    "        cc01_test = pd.read_csv('data/cc_01_test.csv', sep=',')\n",
    "        cc02 = pd.read_csv('data/cc_02.csv', sep=',')\n",
    "        cc02_test = pd.read_csv('data/cc_02_test.csv', sep=',')\n",
    "        cc03 = pd.read_csv('data/cc_03.csv', sep=',')\n",
    "        cc03_test = pd.read_csv('data/cc_03_test.csv', sep=',')\n",
    "        train_data = pd.concat([df_train[df_train.columns.values[:-1]], \n",
    "                                cc01, cc02, cc03, df_train['price']], axis=1)\n",
    "        test_data = pd.concat([df_test, cc01_test, cc02_test, cc03_test], axis=1)\n",
    "        # check_nan(train_data, 5)\n",
    "        # check_nan(test_data, 5)\n",
    "        train_data.to_csv('data/train_concat_location.csv', index = False)\n",
    "        test_data.to_csv('data/test_concat_location.csv', index = False)\n",
    "\n",
    "    elif aspect == 'transportation':\n",
    "        train_01 = pd.read_csv('data/train_01.csv', sep=',')\n",
    "        train_01_test = pd.read_csv('data/train_01_test.csv', sep=',')\n",
    "        train_02 = pd.read_csv('data/train_02.csv', sep=',')\n",
    "        train_02_test = pd.read_csv('data/train_02_test.csv', sep=',')\n",
    "        train_03 = pd.read_csv('data/train_03.csv', sep=',')\n",
    "        train_03_test = pd.read_csv('data/train_03_test.csv', sep=',')\n",
    "        train_data = pd.concat([df_train[df_train.columns.values[:-1]], \n",
    "                                train_01, train_02, train_03, df_train['price']], axis=1)\n",
    "        test_data = pd.concat([df_test, train_01_test, train_02_test, train_03_test], axis=1)\n",
    "        # check_nan(train_data, 5)\n",
    "        # check_nan(test_data, 5)\n",
    "        train_data.to_csv('data/train_concat_transportation.csv', index = False)\n",
    "        test_data.to_csv('data/test_concat_transportation.csv', index = False)\n",
    "    else:\n",
    "        print('Invalid InputError: please check the aspect name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053fdf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25714, 16), (7500, 15))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_data_cleaned.csv', sep=',')\n",
    "df_test = pd.read_csv('data/test_data_cleaned.csv', sep=',')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6aa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_aspect(df_train, df_test, 'schools')\n",
    "concat_aspect(df_train, df_test, 'amenities')\n",
    "concat_aspect(df_train, df_test, 'location')\n",
    "concat_aspect(df_train, df_test, 'transportation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f2dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "def read_data(aspect):\n",
    "    if aspect in ['basic','schools','amenities','location','transportation']:\n",
    "        # concat different aspects\n",
    "        if aspect == 'basic':\n",
    "            train_data = TabularDataset('data/train_data_cleaned.csv')\n",
    "            test_data = TabularDataset('data/test_data_cleaned.csv')\n",
    "        else:\n",
    "            train_data = TabularDataset('data/train_concat_' + aspect + '.csv')\n",
    "            test_data = TabularDataset('data/test_concat_'+ aspect +'.csv')\n",
    "        return train_data, test_data\n",
    "    else:\n",
    "        print('Invalid InputError: please check the aspect name.')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8183ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: data/train_data_cleaned.csv | Columns = 16 / 16 | Rows = 25714 -> 25714\n",
      "Loaded data from: data/test_data_cleaned.csv | Columns = 15 / 15 | Rows = 7500 -> 7500\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220407_104203/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220407_104203/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    25714\n",
      "Train Data Columns: 16\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (74800000.0, 556600.0, 2992606.73952, 4255803.28904)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5456.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.77 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25714, 17) (7500, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['street']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 96\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 8 | ['bedrooms', 'bathrooms', 'lat', 'lng', 'built_year', ...]\n",
      "\t\t('int', [])          : 2 | ['district', 'tenure']\n",
      "\t\t('object', [])       : 5 | ['name', 'model', 'region', 'planning_area', 'subszone']\n",
      "\t\t('object', ['text']) : 1 | ['street']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  5 | ['name', 'model', 'region', 'planning_area', 'subszone']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['street']\n",
      "\t\t('float', [])                       :  8 | ['bedrooms', 'bathrooms', 'lat', 'lng', 'built_year', ...]\n",
      "\t\t('int', [])                         :  1 | ['district']\n",
      "\t\t('int', ['binned', 'text_special']) :  7 | ['street.char_count', 'street.word_count', 'street.lower_ratio', 'street.digit_ratio', 'street.special_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :  1 | ['tenure']\n",
      "\t\t('int', ['text_ngram'])             : 97 | ['__nlp__.10', '__nlp__.11', '__nlp__.12', '__nlp__.13', '__nlp__.15', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t16 features in original data used to generate 120 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.91s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.09722330248113868, Train Rows: 23214, Val Rows: 2500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1314222.2772\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1152118.6437\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "/Users/yanzehong/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 509018\tvalid_set's rmse: 891219\n",
      "[2000]\ttrain_set's rmse: 407114\tvalid_set's rmse: 843994\n",
      "[3000]\ttrain_set's rmse: 346240\tvalid_set's rmse: 808886\n",
      "[4000]\ttrain_set's rmse: 315416\tvalid_set's rmse: 792994\n",
      "[5000]\ttrain_set's rmse: 293219\tvalid_set's rmse: 776120\n",
      "[6000]\ttrain_set's rmse: 275580\tvalid_set's rmse: 761878\n",
      "[7000]\ttrain_set's rmse: 262513\tvalid_set's rmse: 753790\n",
      "[8000]\ttrain_set's rmse: 251586\tvalid_set's rmse: 746145\n",
      "[9000]\ttrain_set's rmse: 242583\tvalid_set's rmse: 738841\n",
      "[10000]\ttrain_set's rmse: 235352\tvalid_set's rmse: 734022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-733945.5397\t = Validation score   (root_mean_squared_error)\n",
      "\t14.86s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yanzehong/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-744021.2754\t = Validation score   (root_mean_squared_error)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-851406.1279\t = Validation score   (root_mean_squared_error)\n",
      "\t12.94s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-612777.254\t = Validation score   (root_mean_squared_error)\n",
      "\t23.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-848284.0469\t = Validation score   (root_mean_squared_error)\n",
      "\t8.57s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-738020.1363\t = Validation score   (root_mean_squared_error)\n",
      "\t30.38s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-935253.1804\t = Validation score   (root_mean_squared_error)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ...\n",
      "\t-662131.1538\t = Validation score   (root_mean_squared_error)\n",
      "\t137.22s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "/Users/yanzehong/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-888343.0301\t = Validation score   (root_mean_squared_error)\n",
      "\t5.06s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-565252.6243\t = Validation score   (root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 243.3s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220407_104203/\")\n"
     ]
    }
   ],
   "source": [
    "# fit and predict\n",
    "aspect = 'basic' # 'basic', 'schools', 'amenities','location','transportation'\n",
    "log_tf = False # True or False, if True, use RMSLE for evaluation, else RMSE\n",
    "\n",
    "train_data, test_data = read_data(aspect)\n",
    "if aspect == 'schools': # keep primary_school_0to1, sec_school_0to3\n",
    "    train_data = train_data.drop(columns=['primary_school_0to2', 'primary_school_0to3', 'sec_school_0to1', 'sec_school_0to2'])\n",
    "    test_data = test_data.drop(columns=['primary_school_0to2', 'primary_school_0to3', 'sec_school_0to1', 'sec_school_0to2'])\n",
    "elif aspect == 'amenities': # keep shopping_0to2, hawker_0to3\n",
    "    train_data = train_data.drop(columns=['shopping_0to1', 'shopping_0to3', 'hawker_0to1', 'hawker_0to2'])\n",
    "    test_data = test_data.drop(columns=['shopping_0to1', 'shopping_0to3', 'hawker_0to1', 'hawker_0to2'])\n",
    "elif aspect == 'location': # keep cc_0to3\n",
    "    train_data = train_data.drop(columns=['cc_0to1', 'cc_0to2'])\n",
    "    test_data = test_data.drop(columns=['cc_0to1', 'cc_0to2'])\n",
    "elif aspect == 'transportation': # keep train_0to3\n",
    "    train_data = train_data.drop(columns=['train_0to1', 'train_0to2'])\n",
    "    test_data = test_data.drop(columns=['train_0to1', 'train_0to2'])\n",
    "\n",
    "# incorporate new feature\n",
    "train_data['bed_bath_ratio'] = train_data['bedrooms']/train_data['bathrooms']\n",
    "test_data['bed_bath_ratio'] = test_data['bedrooms']/test_data['bathrooms']\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "label = 'price'\n",
    "if log_tf:\n",
    "    train_data['price'] = np.log(train_data['price'])\n",
    "    predictor = TabularPredictor(label=label).fit(train_data)\n",
    "else:\n",
    "    predictor = TabularPredictor(label=label).fit(train_data)\n",
    "    \n",
    "preds = predictor.predict(test_data)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "if log_tf:\n",
    "    submission = pd.DataFrame({'Id': df_id['Id'], 'Predicted': np.exp(preds)})\n",
    "    submission.to_csv('results/submission_autogluon_'+aspect+'_newf_log.csv', index = False)\n",
    "else:\n",
    "    submission = pd.DataFrame({'Id': df_id['Id'], 'Predicted': preds})\n",
    "    submission.to_csv('results/submission_autogluon_'+aspect+'_newf.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
