{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine type and model\n",
    "def combine_type_model(df_in, train_set):\n",
    "    df_cleaned = df_in.copy()\n",
    "    condo_type_all=['condominium','apartment','executive condominium','walk-up apt']\n",
    "    k=df_cleaned [df_cleaned ['model'].isna()]\n",
    "    indices_model =k.index\n",
    "    # Fill missing values by ‘name’ using majority vote. If there is no same community, fill in by ‘type’.\n",
    "    for i in indices_model:\n",
    "        condo_type,condo_model,condo_name=df_cleaned.loc[i]['type'],df_cleaned.loc[i]['model'],df_cleaned.loc[i]['name']\n",
    "        df_filter =df_cleaned[(df_cleaned ['model']==df_cleaned ['type'])&(df_cleaned['name'] == condo_name)]\n",
    "        df_filter_dropdup=df_filter.drop_duplicates('model')\n",
    "        if  df_filter_dropdup.shape[0] == 1 :\n",
    "            df_cleaned.loc[i,'model']=df_filter_dropdup.iloc[0]['model']\n",
    "        elif df_filter.shape[0] == 0:\n",
    "            df_cleaned.loc[i,'model']=df_cleaned.loc[i,'type']\n",
    "        elif df_filter.shape[0] > 1:\n",
    "            df_cleaned.loc[i,'model']=df_filter['model'].value_counts().index[0]\n",
    "\n",
    "\n",
    "    # Check conflicting records between ‘type’ and ‘model’. If  ‘model’ value is not a subset of  ‘type’, classify the ‘model’ by its neighbours.  \n",
    "    k=df_cleaned [(df_cleaned ['type'] !=df_cleaned ['model'])]#extract all rows going to process\n",
    "    k1 = k.dropna(subset=['model'])#first ignore the empty value and do the easy task(combine executive condominium and walk-up apt)\n",
    "    indices_model = k1.index\n",
    "    for i in indices_model:\n",
    "        condo_type,condo_model,condo_name=df_cleaned.loc[i]['type'],df_cleaned.loc[i]['model'],df_cleaned.loc[i]['name']\n",
    "        if condo_type in condo_model:\n",
    "            df_cleaned.loc[i,'type']=condo_model\n",
    "        elif condo_model=='walk-up apt':\n",
    "            df_cleaned.loc[i,'type']=condo_model\n",
    "        elif condo_model not in condo_type_all:\n",
    "            df_cleaned.loc[i,'model']=df_cleaned.loc[i,'type']\n",
    "        else:\n",
    "            df_filter = df_cleaned[df_cleaned['name'] == condo_name]\n",
    "            df_filter_dropdup=df_filter[df_filter['type']==df_filter['model']].drop_duplicates('type')\n",
    "            if df_filter_dropdup.shape[0] == 1 :\n",
    "                df_cleaned.loc[i,'type']=df_filter_dropdup.iloc[0]['type']\n",
    "                df_cleaned.loc[i,'model']=df_filter_dropdup.iloc[0]['type']\n",
    "            elif df_filter_dropdup.shape[0]== 0:\n",
    "                df_cleaned.loc[i,'model']=df_cleaned.loc[i,'type']\n",
    "            elif df_filter_dropdup.shape[0]> 1:\n",
    "                df_cleaned.loc[i,'model']=df_filter['model'].value_counts().index[0]\n",
    "                df_cleaned.loc[i,'type']=df_cleaned.loc[i,'model']\n",
    "\n",
    "    # Correct the potential noisy records in ‘model’ value via majority vote by ‘name’.\n",
    "    indice_not=[]\n",
    "    indices_model = df_cleaned.index\n",
    "    for i in indices_model:\n",
    "        condo_type,condo_model,condo_name=df_cleaned.loc[i]['type'],df_cleaned.loc[i]['model'],df_cleaned.loc[i]['name']\n",
    "        df_name=df_cleaned[(df_cleaned['name']==condo_name)]\n",
    "        df_namefilter=df_name[df_name['type']!=condo_type]\n",
    "        if (df_namefilter.drop_duplicates('type').shape[0]>1):\n",
    "            df_cleaned.loc[i,'type']=df_name['model'].value_counts().index[0]\n",
    "            df_cleaned.loc[i,'model']=df_cleaned.loc[i,'type']\n",
    "            indice_not.append(i)\n",
    "    number=len(indice_not)\n",
    "    return df_cleaned, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_median(df_in, target, relate, k):\n",
    "    df = df_in.copy()\n",
    "    indices_target = df.index[df[target].isna()]\n",
    "    indices_length = indices_target.shape[0]\n",
    "    if indices_length == 0:\n",
    "        print('No missing value')\n",
    "        return\n",
    "    relate_r = df[~df[target].isna()][relate].to_numpy()\n",
    "    target_r = df[~df[target].isna()][target].to_numpy()\n",
    "    relate_r_t = np.tile(relate_r, (indices_length,1))\n",
    "    target_r_t = np.tile(target_r, (indices_length,1))\n",
    "    relate_w = df.loc[indices_target, relate].to_numpy()\n",
    "    relate_w = relate_w.reshape(indices_length,1)\n",
    "    diff = np.abs(relate_r_t-relate_w)\n",
    "    k_nearest_indices = np.argsort(diff, axis = 1)[:,:k]\n",
    "    k_targets = target_r_t[:, k_nearest_indices[:,:]]\n",
    "    medians = np.ceil(np.median(k_targets[0,:,:], axis = 1)).astype(int)\n",
    "    df.loc[indices_target, target] = medians\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_median_by_str(df_in, target, relate):\n",
    "    df_cleaned = df_in.copy()\n",
    "    indices_target_na = df_cleaned.index[df_cleaned[target].isna()]\n",
    "    target_no_na = df_cleaned.dropna(subset=[target])\n",
    "    medians, indices_keep, indices_drop = [], [], []\n",
    "    for i in indices_target_na:\n",
    "        condo_relate = df_cleaned.loc[i][relate]\n",
    "        df_filter = target_no_na[(target_no_na[relate] == condo_relate)]\n",
    "        if df_filter.shape[0] == 0:\n",
    "            indices_drop.append(i)\n",
    "            continue\n",
    "        df_filter = df_filter[target].to_numpy()\n",
    "        medians.append(np.median(df_filter))\n",
    "        indices_keep.append(i)\n",
    "    # df_cleaned = df_cleaned.drop(indices_drop, axis=0)\n",
    "    df_cleaned.loc[indices_keep, target] = medians\n",
    "    return df_cleaned, indices_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance matrix for different auxiliary data\n",
    "def location_attr(df, df_target):\n",
    "    df_lat, df_lng = df['lat'].to_numpy(), df['lng'].to_numpy()\n",
    "    df_target_lat, df_target_lng = df_target['lat'].to_numpy(), df_target['lng'].to_numpy()\n",
    "    distances_mat = np.zeros((len(df), len(df_target)))\n",
    "    for i in tqdm_notebook(range(len(df))):\n",
    "        loc_house=(df_lat[i], df_lng[i])\n",
    "        for j in range(len(df_target)):\n",
    "            loc_target=(df_target_lat[j], df_target_lng[j])\n",
    "            res = distance.distance(loc_house, loc_target).km\n",
    "            distances_mat[i][j] = res\n",
    "    df_distance = pd.DataFrame(distances_mat, columns=[df_target['name'].values])\n",
    "    return distances_mat, df_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of target feature\n",
    "def create_target_num(distances, lower, upper, name_str):\n",
    "    target_num = np.zeros((distances.shape[0]))\n",
    "    for r in range(distances.shape[0]):\n",
    "        for c in range(distances.shape[1]):\n",
    "            if lower<=distances[r][c]<=upper: target_num[r] += 1\n",
    "    col_name = name_str + str(lower)+'to'+str(upper)\n",
    "    df_target_num = pd.DataFrame(target_num, columns=[col_name])\n",
    "    return df_target_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(df, k):\n",
    "    total_na = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = ((df.isnull().sum() / df.isnull().count()) * 100).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total_na, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data = missing_data.reset_index()\n",
    "    missing_data.columns = ['Name', 'Total', 'Percent']\n",
    "    print(missing_data[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, train_set):\n",
    "    df_cleaned = df.copy()\n",
    "    if train_set:\n",
    "        factors = ['name','street','type','model','bedrooms', 'bathrooms','district','region','tenure','built_year','no_of_units','area_size','price','date_listed']\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=factors)\n",
    "        print('After removing duplicates, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    if train_set: \n",
    "        df_cleaned = df_cleaned.drop([2168,2415,4402,12891,13181,16294,19026,22378])\n",
    "        print('After removing outliers, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    indices = df_cleaned['bedrooms'].str.contains('\\+', na=False)\n",
    "    rooms = df_cleaned.bedrooms[indices].to_numpy()\n",
    "    f = lambda x: int(x[0]) + int(x[2])\n",
    "    rooms = [f(r) for r in rooms]\n",
    "    df_cleaned.loc[indices, 'bedrooms'] = rooms\n",
    "    indices_notna = df_cleaned.bedrooms.notna()\n",
    "    df_cleaned.loc[indices_notna, 'bedrooms'] = df_cleaned[df_cleaned.bedrooms.notna()].bedrooms.astype(str).astype(int)\n",
    "    df_cleaned = fill_median(df_cleaned, 'bathrooms', 'area_size', 100)\n",
    "    df_cleaned = fill_median(df_cleaned, 'bedrooms', 'area_size', 100)\n",
    "    df_cleaned['bedrooms'] = df_cleaned.bedrooms.astype(str).astype(float)\n",
    "    print('After filling bedrooms and bathrooms attributes, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    df_cleaned, miss_num = combine_type_model(df_cleaned, train_set)\n",
    "    print('After combining model and type attributes, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    indices_tenure_na = df_cleaned['tenure'].isna()\n",
    "    df_cleaned.loc[indices_tenure_na, 'tenure'] = 'leasehold'\n",
    "    indices_freehold = df_cleaned['tenure'].str.contains('freehold|929|946|947|956|998|999|9999', regex=True)\n",
    "    df_cleaned.loc[indices_freehold, 'tenure'] = 1\n",
    "    df_cleaned.loc[~indices_freehold, 'tenure'] = 0\n",
    "    df_cleaned['tenure'] = df_cleaned.tenure.astype(str).astype(int)\n",
    "    print('After classifying tenure, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    df_cleaned, indice_drop = fill_median_by_str(df_cleaned, 'built_year', 'name')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop)))\n",
    "    df_cleaned, indice_drop = fill_median_by_str(df_cleaned, 'built_year', 'street')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop)))\n",
    "    df_cleaned, indice_drop1 = fill_median_by_str(df_cleaned, 'built_year', 'district')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop1)))\n",
    "    print('After filling built_year attributes, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    df_cleaned, indice_drop = fill_median_by_str(df_cleaned, 'no_of_units', 'name')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop)))\n",
    "    df_cleaned, indice_drop = fill_median_by_str(df_cleaned, 'no_of_units', 'street')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop)))\n",
    "    df_cleaned, indice_drop1 = fill_median_by_str(df_cleaned, 'no_of_units', 'district')\n",
    "    print('{} records that are unable to fill.'.format(len(indice_drop1)))\n",
    "    print('After filling no_of_units attributes, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    irrelevants = ['listing_id','date_listed','type','market_segment','type_of_area','eco_category','accessibility']\n",
    "    df_cleaned = df_cleaned.drop(columns=irrelevants)\n",
    "    print('After dropping irrelevant features, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    \n",
    "    print('Finally, there are now {} records, each with {} attributes.'.format(df_cleaned.shape[0], df_cleaned.shape[1]))\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26048 data points, each with 23 attributes.\n",
      "After removing duplicates, there are now 25722 records, each with 23 attributes.\n",
      "After removing outliers, there are now 25714 records, each with 23 attributes.\n",
      "After filling bedrooms and bathrooms attributes, there are now 25714 records, each with 23 attributes.\n",
      "After combining model and type attributes, there are now 25714 records, each with 23 attributes.\n",
      "After classifying tenure, there are now 25714 records, each with 23 attributes.\n",
      "2642 records that are unable to fill.\n",
      "1350 records that are unable to fill.\n",
      "0 records that are unable to fill.\n",
      "After filling built_year attributes, there are now 25714 records, each with 23 attributes.\n",
      "1011 records that are unable to fill.\n",
      "503 records that are unable to fill.\n",
      "0 records that are unable to fill.\n",
      "After filling no_of_units attributes, there are now 25714 records, each with 23 attributes.\n",
      "After dropping irrelevant features, there are now 25714 records, each with 16 attributes.\n",
      "Finally, there are now 25714 records, each with 16 attributes.\n",
      "        Name  Total  Percent\n",
      "0       name      0      0.0\n",
      "1     street      0      0.0\n",
      "2      model      0      0.0\n",
      "3   bedrooms      0      0.0\n",
      "4  bathrooms      0      0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name              object\n",
       "street            object\n",
       "model             object\n",
       "bedrooms         float64\n",
       "bathrooms        float64\n",
       "district           int64\n",
       "region            object\n",
       "planning_area     object\n",
       "subszone          object\n",
       "lat              float64\n",
       "lng              float64\n",
       "tenure             int64\n",
       "built_year       float64\n",
       "no_of_units      float64\n",
       "area_size        float64\n",
       "price            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train data + preprocess\n",
    "df = pd.read_csv('train.csv', sep=',')\n",
    "num_points, num_attributes = df.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_points, num_attributes))\n",
    "df_cleaned = preprocess(df, True)\n",
    "check_nan(df_cleaned, 5)\n",
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment                13708\n",
      "condominium              11364\n",
      "executive condominium      594\n",
      "walk-up apt                 48\n",
      "Name: model, dtype: int64\n",
      "central region       17439\n",
      "north-east region     2755\n",
      "east region           2532\n",
      "west region           2148\n",
      "north region           840\n",
      "Name: region, dtype: int64\n",
      "0    15504\n",
      "1    10210\n",
      "Name: tenure, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check counts for non-numerical features\n",
    "check_attributes = ['model', 'region', 'tenure']\n",
    "for att in check_attributes:\n",
    "    print(df_cleaned[att].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('data/train_data_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7500 data points, each with 22 attributes.\n",
      "After filling bedrooms and bathrooms attributes, there are now 7500 records, each with 22 attributes.\n",
      "After combining model and type attributes, there are now 7500 records, each with 22 attributes.\n",
      "After classifying tenure, there are now 7500 records, each with 22 attributes.\n",
      "1333 records that are unable to fill.\n",
      "900 records that are unable to fill.\n",
      "0 records that are unable to fill.\n",
      "After filling built_year attributes, there are now 7500 records, each with 22 attributes.\n",
      "312 records that are unable to fill.\n",
      "159 records that are unable to fill.\n",
      "0 records that are unable to fill.\n",
      "After filling no_of_units attributes, there are now 7500 records, each with 22 attributes.\n",
      "After dropping irrelevant features, there are now 7500 records, each with 15 attributes.\n",
      "Finally, there are now 7500 records, each with 15 attributes.\n"
     ]
    }
   ],
   "source": [
    "# read test data and preprocess\n",
    "df_test = pd.read_csv('test.csv', sep=',')\n",
    "num_points, num_attributes = df_test.shape\n",
    "print(\"There are {} data points, each with {} attributes.\". format(num_points, num_attributes))\n",
    "noise_index = df_test[df_test['bedrooms']==\"10+\"].index\n",
    "df_test.loc[noise_index[0],'bedrooms']=10\n",
    "df_test_cleaned = preprocess(df_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment                3980\n",
      "condominium              3331\n",
      "executive condominium     169\n",
      "walk-up apt                20\n",
      "Name: model, dtype: int64\n",
      "central region       5080\n",
      "north-east region     801\n",
      "east region           717\n",
      "west region           647\n",
      "north region          255\n",
      "Name: region, dtype: int64\n",
      "0    4439\n",
      "1    3061\n",
      "Name: tenure, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check counts for non-numerical features\n",
    "check_attributes = ['model', 'region', 'tenure']\n",
    "for att in check_attributes:\n",
    "    print(df_test_cleaned[att].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned.to_csv('data/test_data_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg-primary-schools\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c02ea6e1cf41fd964c6f358f8274a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4be6e6153c46b2987fec9ed4815999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sg-secondary-schools\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c9e1784308429d9e9c77b4e633c09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe35da708824d20878ead4c52358f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sg-train-stations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093b29584fd04e1f96b7c0277f1cc993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43d823bb3af40af8b348b7e43d7679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sg-gov-markets-hawker-centres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edd390204b342dfa654d0465091952d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f270529cedb47d09329c10f505c9fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sg-shopping-malls\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a108fe952acc47679032eaf5268315f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1400629f6020485d832540f090b335be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sg-commerical-centres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341b05262cc24dd0ac113b9705fe8c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25714), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615819217950496bb34372db65b2075e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import all auxiliary features automatically\n",
    "# 'pip install geopy' first to convert location\n",
    "aux_files = ['sg-primary-schools','sg-secondary-schools','sg-train-stations','sg-gov-markets-hawker-centres','sg-shopping-malls','sg-commerical-centres']\n",
    "target_names = ['primary_school_','sec_school_','train_','hawker_','shopping_','cc_']\n",
    "df_cleaned = pd.read_csv('data/train_data_cleaned.csv', sep=',')\n",
    "df_test_cleaned = pd.read_csv('data/test_data_cleaned.csv', sep=',')\n",
    "for i in range(len(aux_files)):\n",
    "    print(aux_files[i])\n",
    "    df_target = pd.read_csv('auxiliary-data/auxiliary-data/'+aux_files[i]+'.csv', sep=',') \n",
    "    distances_mat, df_distance =location_attr(df_cleaned, df_target)\n",
    "    df_distance.to_csv('dm_'+aux_files[i]+'.csv', index = False) \n",
    "    target_name = target_names[i]\n",
    "    num_tar_01 = create_target_num(distances_mat, 0, 1, target_name)\n",
    "    num_tar_01.to_csv(target_name+'01.csv', index = False)\n",
    "    num_tar_02 = create_target_num(distances_mat, 0, 2, target_name)\n",
    "    num_tar_02.to_csv(target_name+'02.csv', index = False)\n",
    "    num_tar_03 = create_target_num(distances_mat, 0, 3, target_name)\n",
    "    num_tar_03.to_csv(target_name+'03.csv', index = False)\n",
    "    \n",
    "    \n",
    "    distances_mat_test, df_distance_test =location_attr(df_test_cleaned, df_target)\n",
    "    df_distance_test.to_csv('dm_'+aux_files[i]+'_test.csv', index = False) \n",
    "    num_tar_01_test = create_target_num(distances_mat_test, 0, 1, target_name)\n",
    "    num_tar_01_test.to_csv(target_name+'01_test.csv', index = False)\n",
    "    num_tar_02_test = create_target_num(distances_mat_test, 0, 2, target_name)\n",
    "    num_tar_02_test.to_csv(target_name+'02_test.csv', index = False)\n",
    "    num_tar_03_test = create_target_num(distances_mat_test, 0, 3, target_name)\n",
    "    num_tar_03_test.to_csv(target_name+'03_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
